import random
import math

# Sigmoid activation function
def sigmoid(x):
    return 1 / (1 + math.exp(-x))

# อนุพันธ์ของฟังก์ชัน Sigmoid
def dsigmoid(y):
    return y * (1 - y)

# Multi-Layer Perceptron class (MLP)
class MLP:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.5, momentum=0.9, weight_init_value=0.5):
        # กำหนดค่าเริ่มต้นของตัวแปร input_size, hidden_size, output_size, learning_rate, momentum
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.lr = learning_rate  # ค่า learning rate ใช้ในการควบคุมการปรับน้ำหนัก
        self.momentum = momentum  # ค่า momentum ช่วยในการคำนวณการอัพเดตน้ำหนัก

        # กำหนดน้ำหนัก (weights) ของการเชื่อมต่อระหว่าง input layer และ hidden layer
        self.w_input_hidden = self.initialize_weights(input_size + 1, hidden_size, weight_init_value)
        # กำหนดน้ำหนักของการเชื่อมต่อระหว่าง hidden layer และ output layer
        self.w_hidden_output = self.initialize_weights(hidden_size + 1, output_size, weight_init_value)

        # กำหนด momentum deltas สำหรับน้ำหนักในทั้งสองชั้น
        self.delta_w_input_hidden_prev = [[0] * hidden_size for _ in range(input_size + 1)]
        self.delta_w_hidden_output_prev = [[0] * output_size for _ in range(hidden_size + 1)]

    def initialize_weights(self, size_in, size_out, weight_init_value):
        # ฟังก์ชันนี้จะทำการสร้างและตั้งค่าน้ำหนักเริ่มต้น
        # น้ำหนักถูกตั้งค่าเป็นค่าคงที่ (weight_init_value) สำหรับการเชื่อมต่อทั้งหมด
        return [[weight_init_value] * size_out for _ in range(size_in)]

    def forward(self, inputs):
        # ฟังก์ชันนี้จะคำนวณค่า outputs ของแต่ละ layer โดยใช้ฟังก์ชันการกระตุ้น (activation function)
        
        self.inputs = inputs + [1.0]  # เพิ่มค่า bias ให้กับ input (ค่า bias คือ 1.0)
        
        # คำนวณค่า hidden layer โดยใช้ sigmoid activation function
        self.hidden = [sigmoid(sum(self.inputs[i] * self.w_input_hidden[i][j] for i in range(self.input_size + 1)))
                       for j in range(self.hidden_size)]
        self.hidden.append(1.0)  # เพิ่มค่า bias ที่ hidden layer

        # คำนวณค่า output layer โดยใช้ sigmoid activation function
        self.outputs = [sigmoid(sum(self.hidden[j] * self.w_hidden_output[j][k] for j in range(self.hidden_size + 1)))
                        for k in range(self.output_size)]
        
        # คืนค่าผลลัพธ์ของ output layer
        return self.outputs

    def backward(self, targets):
        # ฟังก์ชันนี้ใช้ในการคำนวณค่าความผิดพลาด (errors) และอัพเดตน้ำหนัก (weights)

        # คำนวณค่าความผิดพลาดที่ output layer โดยใช้การคำนวณจาก target และ output ที่ได้
        output_errors = [(targets[k] - self.outputs[k]) * dsigmoid(self.outputs[k]) for k in range(self.output_size)]
        
        # คำนวณค่าความผิดพลาดที่ hidden layer โดยใช้ผลจาก output layer
        hidden_errors = [sum(output_errors[k] * self.w_hidden_output[j][k] for k in range(self.output_size)) * dsigmoid(self.hidden[j])
                         for j in range(self.hidden_size)]

        # อัพเดตน้ำหนักที่ hidden-output layer โดยใช้ค่า momentum และ learning rate
        for j in range(self.hidden_size + 1):
            for k in range(self.output_size):
                # คำนวณการอัพเดตน้ำหนัก
                delta = self.lr * output_errors[k] * self.hidden[j] + self.momentum * self.delta_w_hidden_output_prev[j][k]
                self.w_hidden_output[j][k] += delta
                # เก็บค่า delta เพื่อใช้ใน momentum
                self.delta_w_hidden_output_prev[j][k] = delta

        # อัพเดตน้ำหนักที่ input-hidden layer
        for i in range(self.input_size + 1):
            for j in range(self.hidden_size):
                # คำนวณการอัพเดตน้ำหนัก
                delta = self.lr * hidden_errors[j] * self.inputs[i] + self.momentum * self.delta_w_input_hidden_prev[i][j]
                self.w_input_hidden[i][j] += delta
                # เก็บค่า delta เพื่อใช้ใน momentum
                self.delta_w_input_hidden_prev[i][j] = delta

    def train(self, training_data, epochs=1000):
        # ฟังก์ชันนี้ใช้ในการฝึกฝนโมเดลโดยใช้ข้อมูล training data และจำนวน epoch ที่กำหนด

        for epoch in range(epochs):
            total_error = 0.0
            # วนลูปการฝึกฝนในแต่ละตัวอย่าง (inputs, targets) จาก training data
            for inputs, targets in training_data:
                # ทำการคำนวณ forward pass เพื่อหาค่าผลลัพธ์จาก inputs
                outputs = self.forward(inputs)
                # คำนวณค่า error และทำการคำนวณ backward pass เพื่ออัพเดตน้ำหนัก
                self.backward(targets)
                # คำนวณค่า error สำหรับตัวอย่างนี้ (MSE: Mean Squared Error)
                total_error += sum((targets[k] - outputs[k])**2 for k in range(len(targets)))

# Flood dataset preparation (8 inputs: station 1 and station 2 data, 1 output: predicted water level)
flood_data_set = [
    [95,95,95,95,148,149,150,150,153],
    [95,95,95,95,149,150,150,150,153],
    [95,95,95,95,150,150,150,150,153],
    [95,95,95,95,150,150,150,150,153],
    [95,95,95,95,150,150,150,152,153],
    [95,95,95,95,150,150,152,152,153],
    [95,95,95,96,150,152,152,153,153],
    [95,95,96,97,152,152,153,153,153],
    [95,96,97,98,152,153,153,153,153],
    [96,97,98,100,153,153,153,153,154],
    [97,98,100,100,153,153,153,153,155],
    [98,100,100,100,153,153,153,153,156],
    [100,100,100,101,153,153,153,153,156],
    [100,100,101,101,153,153,153,153,156],
    [100,101,101,102,153,153,153,153,156],
    [101,101,102,102,153,153,153,153,160],
    [101,102,102,103,153,153,153,154,164],
    [102,102,103,104,153,153,154,155,165],
    [102,103,104,105,153,154,155,156,168],
    [103,104,105,106,154,155,156,156,172],
    [104,105,106,109,155,156,156,156,178],
    [105,106,109,113,156,156,156,156,188],
    [106,109,113,120,156,156,156,160,192],
    [109,113,120,134,156,156,160,164,207],
    [113,120,134,143,156,160,164,165,224],
    [120,134,143,162,160,164,165,168,237],
    [134,143,162,184,164,165,168,172,246],
    [143,162,184,204,165,168,172,178,259],
    [162,184,204,228,168,172,178,188,270],
    [184,204,228,250,172,178,188,192,280],
    [204,228,250,277,178,188,192,207,290],
    [228,250,277,295,188,192,207,224,297],
    [250,277,295,300,192,207,224,237,303],
    [277,295,300,320,207,224,237,246,314],
    [295,300,320,330,224,237,246,259,326],
    [300,320,330,349,237,246,259,270,340],
    [320,330,349,368,246,259,270,280,362],
    [330,349,368,385,259,270,280,290,390],
    [349,368,385,445,270,280,290,297,412],
    [368,385,445,465,280,290,297,303,435],
    [385,445,465,514,290,297,303,314,447],
    [445,465,514,550,297,303,314,326,465],
    [465,514,550,573,303,314,326,340,468],
    [514,550,573,586,314,326,340,362,470],
    [550,573,586,588,326,340,362,390,472],
    [573,586,588,594,340,362,390,412,475],
    [586,588,594,598,362,390,412,435,478],
    [588,594,598,602,390,412,435,447,481],
    [594,598,602,606,412,435,447,465,483],
    [598,602,606,609,435,447,465,468,485],
    [602,606,609,612,447,465,468,470,486],
    [606,609,612,616,465,468,470,472,488],
    [609,612,616,620,468,470,472,475,489],
    [612,616,620,624,470,472,475,478,490],
    [616,620,624,626,472,475,478,481,490],
    [620,624,626,628,475,478,481,483,490],
    [624,626,628,628,478,481,483,485,490],
    [626,628,628,628,481,483,485,486,490],
    [628,628,628,627,483,485,486,488,490],
    [628,628,627,624,485,486,488,489,490],
    [628,627,624,618,486,488,489,490,490],
    [627,624,618,613,488,489,490,490,489],
    [624,618,613,608,489,490,490,490,488],
    [618,613,608,604,490,490,490,490,486],
    [613,608,604,596,490,490,490,490,484],
    [608,604,596,592,490,490,490,490,481],
    [604,596,592,586,490,490,490,490,478],
    [596,592,586,580,490,490,490,490,475],
    [592,586,580,576,490,490,490,489,470],
    [586,580,576,570,490,490,489,488,468],
    [580,576,570,564,490,489,488,486,464],
    [576,570,564,556,489,488,486,484,460],
    [570,564,556,544,488,486,484,481,456],
    [564,556,544,520,486,484,481,478,451],
    [556,544,520,510,484,481,478,475,448],
    [544,520,510,504,481,478,475,470,444],
    [520,510,504,496,478,475,470,468,440],
    [510,504,496,490,475,470,468,464,436],
    [504,496,490,484,470,468,464,460,431],
    [496,490,484,476,468,464,460,456,427],
    [490,484,476,470,464,460,456,451,422],
    [484,476,470,464,460,456,451,448,416],
    [476,470,464,457,456,451,448,444,410],
    [470,464,457,452,451,448,444,440,404],
    [464,457,452,448,448,444,440,436,396],
    [457,452,448,442,444,440,436,431,390],
    [452,448,442,437,440,436,431,427,382],
    [448,442,437,432,436,431,427,422,376],
    [442,437,432,426,431,427,422,416,368],
    [437,432,426,421,427,422,416,410,360],
    [432,426,421,416,422,416,410,404,356],
    [426,421,416,411,416,410,404,396,351],
    [421,416,411,406,410,404,396,390,344],
    [416,411,406,400,404,396,390,382,339],
    [411,406,400,396,396,390,382,376,335],
    [406,400,396,391,390,382,376,368,329],
    [400,396,391,386,382,376,368,360,327],
    [396,391,386,382,376,368,360,356,323],
    [391,386,382,377,368,360,356,351,319],
    [386,382,377,372,360,356,351,344,315],
    [382,377,372,370,356,351,344,339,311],
    [377,372,370,364,351,344,339,335,308],
    [372,370,364,359,344,339,335,329,306],
    [370,364,359,354,339,335,329,327,303],
    [364,359,354,350,335,329,327,323,300],
    [359,354,350,345,329,327,323,319,296],
    [354,350,345,341,327,323,319,315,294],
    [350,345,341,337,323,319,315,311,292],
    [345,341,337,334,319,315,311,308,288],
    [341,337,334,330,315,311,308,306,286],
    [337,334,330,328,311,308,306,303,284],
    [334,330,328,326,308,306,303,300,280],
    [330,328,326,324,306,303,300,296,276],
    [328,326,324,322,303,300,296,294,274],
    [326,324,322,320,300,296,294,292,271],
    [324,322,320,319,296,294,292,288,268],
    [322,320,319,318,294,292,288,286,265],
    [320,319,318,317,292,288,286,284,262],
    [319,318,317,316,288,286,284,280,259],
    [318,317,316,314,286,284,280,276,256],
    [317,316,314,313,284,280,276,274,253],
    [316,314,313,312,280,276,274,271,253],
    [314,313,312,312,276,274,271,268,252],
    [313,312,312,311,274,271,268,265,252],
    [312,312,311,310,271,268,265,262,251],
    [312,311,310,310,268,265,262,259,250],
    [311,310,310,290,265,262,259,256,249],
    [310,310,290,288,262,259,256,253,248],
    [310,290,288,287,259,256,253,253,247],
    [290,288,287,285,256,253,253,252,247],
    [288,287,285,282,253,253,252,252,247],
    [287,285,282,280,253,252,252,251,246],
    [285,282,280,276,252,252,251,250,246],
    [282,280,276,273,252,251,250,249,245],
    [280,276,273,269,251,250,249,248,245],
    [276,273,269,265,250,249,248,247,244],
    [273,269,265,261,249,248,247,247,243],
    [269,265,261,258,248,247,247,247,242],
    [265,261,258,256,247,247,247,246,241],
    [261,258,256,276,247,247,246,246,240],
    [258,256,276,272,247,246,246,245,239],
    [256,276,272,270,246,246,245,245,238],
    [276,272,270,267,246,245,245,244,236],
    [272,270,267,262,245,245,244,243,234],
    [270,267,262,258,245,244,243,242,235],
    [267,262,258,257,244,243,242,241,234],
    [262,258,257,256,243,242,241,240,233],
    [258,257,256,256,242,241,240,239,232],
    [257,256,256,255,241,240,239,238,231],
    [256,256,255,256,240,239,238,236,230],
    [256,255,256,256,239,238,236,234,230],
    [255,256,256,256,238,236,234,235,231],
    [256,256,256,256,236,234,235,234,232],
    [256,256,256,256,234,235,234,233,234],
    [256,256,256,256,235,234,233,232,236],
    [256,256,256,256,234,233,232,231,240],
    [250,250,250,240,215,210,210,209,208],
    [250,250,240,240,210,210,209,209,208],
    [250,240,240,240,210,209,209,209,208],
    [240,240,240,240,209,209,209,209,209],
    [240,240,240,240,209,209,209,209,209],
    [240,240,240,240,209,209,209,208,209],
    [240,240,240,240,209,209,208,208,209],
    [240,240,240,240,209,208,208,208,209],
    [240,240,240,241,208,208,208,208,209],
    [240,240,241,242,208,208,208,208,210],
    [240,241,242,242,208,208,208,209,210],
    [241,242,242,243,208,208,209,209,211],
    [242,242,243,243,208,209,209,209,211],
    [242,243,243,242,209,209,209,209,212],
    [243,243,242,242,209,209,209,209,213],
    [243,242,242,241,209,209,209,209,213],
    [242,242,241,240,209,209,209,210,214],
    [242,241,240,239,209,209,210,210,214],
    [241,240,239,238,209,210,210,211,214],
    [240,239,238,237,210,210,211,211,215],
    [239,238,237,236,210,211,211,212,215],
    [238,237,236,235,211,211,212,213,215],
    [237,236,235,234,211,212,213,213,215],
    [236,235,234,233,212,213,213,214,216],
    [235,234,233,232,213,213,214,214,217],
    [234,233,232,231,213,214,214,214,218],
    [233,232,231,230,214,214,214,215,221],
    [232,231,230,235,214,214,215,215,225],
    [231,230,235,240,214,215,215,215,230],
    [230,235,240,246,215,215,215,215,232],
    [235,240,246,245,215,215,215,216,235],
    [240,246,245,245,215,215,216,217,238],
    [246,245,245,250,215,216,217,218,240],
    [245,245,250,257,216,217,218,221,245],
    [245,250,257,263,217,218,221,225,250],
    [250,257,263,266,218,221,225,230,255],
    [257,263,266,268,221,225,230,232,260],
    [263,266,268,269,225,230,232,235,270],
    [266,268,269,270,230,232,235,238,288],
    [268,269,270,274,232,235,238,240,300],
    [269,270,274,277,235,238,240,245,314],
    [270,274,277,281,238,240,245,250,328],
    [274,277,281,297,240,245,250,255,340],
    [277,281,297,300,245,250,255,260,350],
    [281,297,300,320,250,255,260,270,360],
    [297,300,320,344,255,260,270,288,370],
    [300,320,344,358,260,270,288,300,380],
    [320,344,358,370,270,288,300,314,390],
    [344,358,370,380,288,300,314,328,398],
    [358,370,380,394,300,314,328,340,405],
    [370,380,394,404,314,328,340,350,412],
    [380,394,404,413,328,340,350,360,420],
    [394,404,413,421,340,350,360,370,428],
    [404,413,421,432,350,360,370,380,433],
    [413,421,432,443,360,370,380,390,438],
    [421,432,443,454,370,380,390,398,445],
    [432,443,454,464,380,390,398,405,450],
    [443,454,464,476,390,398,405,412,454],
    [454,464,476,485,398,405,412,420,457],
    [464,476,485,496,405,412,420,428,460],
    [476,485,496,508,412,420,428,433,465],
    [485,496,508,517,420,428,433,438,466],
    [496,508,517,523,428,433,438,445,467],
    [508,517,523,529,433,438,445,450,469],
    [517,523,529,535,438,445,450,454,470],
    [523,529,535,540,445,450,454,457,471],
    [529,535,540,542,450,454,457,460,471],
    [535,540,542,543,454,457,460,465,471],
    [540,542,543,543,457,460,465,466,471],
    [542,543,543,543,460,465,466,467,470],
    [543,543,543,543,465,466,467,469,470],
    [543,543,543,542,466,467,469,470,470],
    [543,543,542,542,467,469,470,471,469],
    [543,542,542,539,469,470,471,471,468],
    [542,542,539,534,470,471,471,471,467],
    [542,539,534,531,471,471,471,471,465],
    [539,534,531,527,471,471,471,470,462],
    [534,531,527,521,471,471,470,470,459],
    [531,527,521,518,471,470,470,470,457],
    [527,521,518,513,470,470,470,469,455],
    [521,518,513,507,470,470,469,468,450],
    [518,513,507,503,470,469,468,467,447],
    [513,507,503,497,469,468,467,465,444],
    [507,503,497,494,468,467,465,462,441],
    [503,497,494,487,467,465,462,459,437],
    [497,494,487,480,465,462,459,457,433],
    [494,487,480,475,462,459,457,455,430],
    [487,480,475,468,459,457,455,450,427],
    [480,475,468,460,457,455,450,447,424],
    [475,468,460,452,455,450,447,444,420],
    [468,460,452,445,450,447,444,441,417],
    [460,452,445,438,447,444,441,437,413],
    [452,445,438,431,444,441,437,433,410],
    [445,438,431,453,441,437,433,430,405],
    [438,431,453,453,437,433,430,427,401],
    [431,453,453,447,433,430,427,424,397],
    [453,453,447,444,430,427,424,420,393],
    [453,447,444,440,427,424,420,417,390],
    [447,444,440,436,424,420,417,413,385],
    [444,440,436,431,420,417,413,410,381],
    [440,436,431,420,417,413,410,405,377],
    [436,431,420,424,413,410,405,401,374],
    [431,420,424,420,410,405,401,397,370],
    [420,424,420,416,405,401,397,393,365],
    [424,420,416,413,401,397,393,390,361],
    [420,416,413,413,397,393,390,385,357],
    [416,413,413,407,393,390,385,381,353],
    [413,413,407,404,390,385,381,377,349],
    [413,407,404,401,385,381,377,374,345],
    [407,404,401,398,381,377,374,370,341],
    [404,401,398,395,377,374,370,365,337],
    [401,398,395,392,374,370,365,361,333],
    [398,395,392,390,370,365,361,357,328],
    [395,392,390,385,365,361,357,353,325],
    [392,390,385,382,361,357,353,349,322],
    [390,385,382,379,357,353,349,345,319],
    [385,382,379,376,353,349,345,341,317],
    [382,379,376,374,349,345,341,337,315],
    [379,376,374,372,345,341,337,333,313],
    [376,374,372,369,341,337,333,328,311],
    [374,372,369,366,337,333,328,325,309],
    [372,369,366,363,333,328,325,322,308],
    [369,366,363,360,328,325,322,319,307],
    [366,363,360,359,325,322,319,317,305],
    [363,360,359,358,322,319,317,315,303],
    [360,359,358,357,319,317,315,313,300],
    [359,358,357,355,317,315,313,311,298],
    [358,357,355,353,315,313,311,309,296],
    [357,355,353,350,313,311,309,308,294],
    [355,353,350,348,311,309,308,307,294],
    [353,350,348,346,309,308,307,305,293],
    [350,348,346,343,308,307,305,303,291],
    [348,346,343,341,307,305,303,300,291],
    [346,343,341,339,305,303,300,298,290],
    [343,341,339,337,303,300,298,296,288],
    [341,339,337,338,300,298,296,294,288],
    [339,337,338,339,298,296,294,294,290],
    [337,338,339,340,296,294,294,293,294],
    [338,339,340,342,294,294,293,291,302],
    [339,340,342,345,294,293,291,291,308],
    [340,342,345,350,293,291,291,290,315],
    [342,345,350,352,291,291,290,288,322],
    [345,350,352,356,291,290,288,288,327],
    [350,352,356,360,290,288,288,290,330],
    [352,356,360,364,288,288,290,294,333],
    [356,360,364,365,288,290,294,302,333],
    [360,364,365,366,290,294,302,308,331],
    [364,365,366,368,294,302,308,315,328],
    [365,366,368,369,302,308,315,322,328],
    [366,368,369,368,308,315,322,327,325],
    [368,369,368,368,315,322,327,330,322],
    [369,368,368,367,322,327,330,333,320],
    [368,368,367,365,327,330,333,333,317],
    [368,367,365,363,330,333,333,331,315],
    [367,365,363,360,333,333,331,328,310],
    [365,363,360,357,333,331,328,328,307],
    [363,360,357,355,331,328,328,325,305],
    [360,357,355,354,328,328,325,322,305]
]

flood_data = [(row[:8], [row[8]]) for row in flood_data_set]  # Convert to inputs and targets

# Normalize dataset
def normalize_dataset(data):
    flat_inputs = [x for sample in data for x in sample[0]]
    flat_outputs = [y[0] for _, y in data]
    min_in, max_in = min(flat_inputs), max(flat_inputs)
    min_out, max_out = min(flat_outputs), max(flat_outputs)

    norm_data = []
    for inputs, target in data:
        norm_inputs = [(x - min_in) / (max_in - min_in) for x in inputs]
        norm_target = [(target[0] - min_out) / (max_out - min_out)]
        norm_data.append((norm_inputs, norm_target))

    return norm_data, min_out, max_out

# 10-fold cross-validation
def k_fold_split(data, k=10):
    random.shuffle(data)
    fold_size = len(data) // k
    return [data[i * fold_size:(i + 1) * fold_size] for i in range(k)]

# Testing different configurations
# (Hidden node , Learning rate , momentum , weight  )
test_configurations = [
    (2, 0.01, 0.7, 0.5),
    (4, 0.01, 0.7, 0.5),
    (6, 0.01, 0.7, 0.5),
    (4, 0.1, 0.9, 1)
    # Add more configurations if necessary
]

norm_data, min_out, max_out = normalize_dataset(flood_data)

# Loop through all configurations
for hidden_nodes, learning_rate, momentum, weight_init_value in test_configurations:
    print(f"Testing hidden_nodes={hidden_nodes}, learning_rate={learning_rate}, momentum={momentum}, weight_init_value={weight_init_value}")
    
    folds = k_fold_split(norm_data, k=10)
    
    fold_mse_scores = []  # Store MSE for each fold
    
    # Iterate through each fold
    for fold_index in range(10):
        test_data = folds[fold_index]
        train_data = [item for i in range(10) if i != fold_index for item in folds[i]]

        # Create and train MLP
        mlp = MLP(input_size=8, hidden_size=hidden_nodes, output_size=1,
                  learning_rate=learning_rate, momentum=momentum, weight_init_value=weight_init_value)
        mlp.train(train_data, epochs=1000)

        total_mse = 0.0
        for inputs, target in test_data:
            prediction = mlp.forward(inputs)[0]
            error = (prediction - target[0]) ** 2
            total_mse += error
        
        avg_mse = total_mse / len(test_data)
        fold_mse_scores.append(avg_mse)

        # Print the MSE for each fold
        print(f"Fold {fold_index + 1}: MSE = {avg_mse:.6f}")
    
    # Calculate average MSE for the current configuration
    avg_mse_all = sum(fold_mse_scores) / len(fold_mse_scores)
    print(f"Average MSE for hidden_nodes={hidden_nodes}, learning_rate={learning_rate}, momentum={momentum}, weight_init_value={weight_init_value}: {avg_mse_all:.6f}\n")

