import os
import math
import random

# Sigmoid Activation Function & derivative  
def sigmoid_function(x):
    return  1.0 / (1.0 + math.exp(-max(min(x, 20), -20)))

def sigmoid_derivative_func(output):
    return output * (1.0 - output)

# -  ดึงข้อมูล flood_data -
def load_flood_data_set(filename):
    data = []
    with open(filename, "r") as file:                  # เปิดไฟล์ข้อมูลเพื่ออ่าน
        for line in file:
            line = line.strip()
            if line == "":
                continue
            try:
                values = list(map(float, line.split()))        # แปลงค่าจากข้อความเป็นตัวเลข
                if len( values ) == 9:                         # รับข้อมูล 9 ค่า
                    data.append(values)
            except ValueError:
                continue
    return data                                                # คืนค่าข้อมูลที่โหลดมา

# -  ดึงข้อมูล cross -
def load_cross_data_set(filename):
    data = []
    with open(filename, "r") as f:
        while True:
            line = f.readline().strip()
            if not line:
                break
            if line.startswith("p"):
                features = list(map(float, f.readline().strip().split()))
                labels = list(map(int, f.readline().strip().split()))
                true_class = 0 if labels[0] == 1 else 1
                data.append(features + [true_class])
    return data




# =-  normalize ข้อมูล
def normalize(data):
    features = [row[:-1] for row in data]  # ฟีเจอร์ (ข้อมูลที่ใช้ในการฝึก)
    target = [row[-1] for row in data]     # เป้าหมาย (ผลลัพธ์ที่ต้องการทำนาย)
    
    min_values = [min(col) for col in zip(*features)]  # ค่าต่ำสุดของฟีเจอร์
    max_values = [max(col) for col in zip(*features)]  # ค่าสูงสุดของฟีเจอร์
    
    normalize_features = [
        [(val - min_values[i]) / (max_values[i] - min_values[i] if max_values[i] != min_values[i]  else 1)
         for i, val in enumerate(row)]
        for row in features
    ]
    
    target_min = min(target)
    target_max = max(target)
    target_range = target_max - target_min if target_max != target_min else 1
    normalize_target = [(val - target_min) / target_range for val in target]
    
    return [normalize_features[i] + [normalize_target[i]] for i in range(len(data))]  , (target_min, target_max, target_range)


#  multilayer perceptron ของ Predict ระดับน้ำ
class MLP_Predictflood:
    def __init__(self, input_node=8, hidden_node=4, output_node=1):
        self.epoch = 1000
        self.learning_rate = 0.01
        self.momentum_rate = 0.9
        self.input_nodes = input_node
        self.output_node = output_node
        self.hidden_node = hidden_node
        
# การเริ่มต้นน้ำหนัก (weights) ระหว่าง input layer และ hidden layer
        self.weg1 = [[random.uniform(-1, 1) for _ in range(input_node)] 
                   for _ in range(hidden_node)]
# การเริ่มต้น bias สำหรับ hidden layer
        self.bias1 = [random.uniform(-1, 1) for _ in range(hidden_node)]
 # การเริ่มต้นน้ำหนัก (weights) ระหว่าง hidden layer และ output layer
        self.weg2 = [[random.uniform(-1, 1) for _ in range(hidden_node)] 
                   for _ in range(output_node)]
# การเริ่มต้น bias สำหรับ output layer
        self.bias2 = [random.uniform(-1, 1) for _ in range(output_node)]
# การเริ่มต้นการเก็บการเปลี่ยนแปลงของน้ำหนักและ bias จากรอบก่อนหน้า (สำหรับโมเมนตัม)
        self.previous_dweg1  = [[0.0 for _ in range(input_node)] for _ in range(hidden_node)]
        self.previous_dbias1 = [0.0 for _ in range(hidden_node)]
        self.previous_dweg2  = [[0.0 for _ in range(hidden_node)] for _ in range(output_node)]
        self.previous_dbias2 = [0.0 for _ in range(output_node)]

    def forward(self, X):
        # คำนวณผลรวมของการคูณน้ำหนักและอินพุต (hidden layer input)
        self.hidden_layer_input = [sum(self.weg1[i][j] * X[j] for j in range(self.input_nodes)) + self.bias1[i] 
                  for i in range(self.hidden_node)]
        # ใช้ฟังก์ชัน sigmoid เพื่อคำนวณค่า activation ของ hidden layer (a1)
        self.a1 = [sigmoid_function(z) for z in self.hidden_layer_input]    # a= activation ของ hidden layer
        # คำนวณผลรวมของการคูณน้ำหนักและผลลัพธ์จาก hidden layer (output layer input)
        self.output_layer_input = [sum(self.weg2[k][i] * self.a1[i] for i in range(self.hidden_node)) + self.bias2[k]
                  for k in range(self.output_node)]
        # คืนค่าผลลัพธ์
        return self.output_layer_input[0] 

    def backward(self, X, y_true, y_pred):
        # คำนวณข้อผิดพลาด (error) ระหว่างค่าทำนายและค่าจริง
        error = y_pred - y_true
        # คำนวณ delta สำหรับ output layer
        delta2 = [error]
        # คำนวณ delta สำหรับ hidden layer
        delta1 = [sum(delta2[k] * 
                  self.weg2[k][i] for k in range(self.output_node)) * 
                  sigmoid_derivative_func(self.a1[i]) for i in range(self.hidden_node)]
        # อัปเดตน้ำหนักและ bias สำหรับ hidden layer
        for i in range(self.hidden_node):
            for j in range(self.input_nodes):
                dweg1 = delta1[i] * X[j]
                self.weg1[i][j] -= self.learning_rate * dweg1 + self.momentum_rate * self.previous_dweg1[i][j]
                self.previous_dweg1[i][j] = dweg1
            self.bias1[i] -= self.learning_rate * delta1[i] + self.momentum_rate * self.previous_dbias1[i]
            self.previous_dbias1[i] = delta1[i]
        # อัปเดตน้ำหนักและ bias สำหรับ output layer
        for k in range(self.output_node):
            for i in range(self.hidden_node):
                dweg2 = delta2[k] * self.a1[i]
                self.weg2[k][i] -= self.learning_rate * dweg2 + self.momentum_rate * self.previous_dweg2[k][i]
                self.previous_dweg2[k][i] = dweg2
        # อัปเดต bias สำหรับ output layer
            self.bias2[k] -= self.learning_rate * delta2[k] + self.momentum_rate * self.previous_dbias2[k]
            self.previous_dbias2[k] = delta2[k]
        
        return error ** 2


def K_Fold_Cross_Validation( data , k=10, hidden_node=5 , learning_r=0.01 , momentum_r=0.9):
    # คำนวณขนาดของแต่ละ fold
    fold_size = len(data) // k
    # สับเปลี่ยนข้อมูลเพื่อให้เกิดความสุ่ม
    random.shuffle(data)
    mse_scores = []  # เก็บค่า Mean Squared Error (MSE) ของแต่ละ fold
    
    for i in range(k):
        # แบ่งข้อมูลเป็น fold สำหรับ validation และ training
        value_data = data[i*fold_size : (i+1)*fold_size]
        train_data = data[:i*fold_size] + data[(i+1)*fold_size:]
        
        mlp = MLP_Predictflood(hidden_node=hidden_node)
        mlp.learning_rate = learning_r
        mlp.momentum_rate = momentum_r
        
        # Training
        for epoch in range(mlp.epoch):
            total_error = 0
            random.shuffle(train_data)
            for sample in train_data:
                X = sample[:-1]
                y_true = sample[-1]
                y_pred = mlp.forward(X)
                error = mlp.backward(X, y_true, y_pred)
                total_error += error
            if total_error / len(train_data) < 1e-6:
                break
        
        # Validation
        total_mse = 0
        for sample in value_data:
            X = sample[:-1]
            y_true = sample[-1]
            y_pred = mlp.forward(X)
            total_mse += (y_true - y_pred)**2
        # คำนวณ MSE สำหรับ fold นี้
        mse = total_mse / len(value_data)*0.1
        mse_scores.append(mse)
        print(f"Fold{i+1} MSE = {mse:.6f}")
    # คำนวณค่าเฉลี่ย MSE ของทุก fold
    average_mse = sum(mse_scores) / len(mse_scores)
    std_mse = math.sqrt(sum((x - average_mse)**2 for x in mse_scores) / len(mse_scores))
    print(f"\nAverage Mean Squared Error (MSE): {average_mse:.6f}")
    
# =--- ทำ confusion_matrix    
def generate_confusion_matrix(true_labels, predicted_labels):
    true_positive = true_negative = false_positive = false_negative = 0  # กำหนดค่าเริ่มต้นของ TP, TN, FP, FN
    
    for true, pred in zip(true_labels, predicted_labels):
        if true == 1 and pred == 1:  # ถ้าค่าจริงเป็น 1 และทำนายเป็น 1
            true_positive += 1
        elif true == 0 and pred == 0:  # ถ้าค่าจริงเป็น 0 และทำนายเป็น 0
            true_negative += 1
        elif true == 0 and pred == 1:  # ถ้าค่าจริงเป็น 0 และทำนายเป็น 1
            false_positive += 1
        elif true == 1 and pred == 0:  # ถ้าค่าจริงเป็น 1 และทำนายเป็น 0
            false_negative += 1
            
    return [[true_positive, false_negative], [false_positive, true_negative]]  # คืนค่า Confusion Matrix


#  multilayer perceptron ของ Classification
class MLP_Classification:
    def __init__(self, input_nodes=2, hidden_node=4, output_node=1):
        self.epoch = 1000           # กำหนดให้ epoch = 1000  
        self.learning_rate = 0.01
        self.momentum_rate = 0.8          
        self.input_nodes = input_nodes
        self.output_node = output_node
        self.hidden_node = hidden_node
        
        # น้ำหนัก (weights) สำหรับเชื่อมต่อระหว่าง input layer และ hidden layer
        self.weg1 = [[random.uniform(-1, 1) for _ in range(input_nodes)] 
                   for _ in range(hidden_node)]
        self.bias1 = [random.uniform(-1, 1) for _ in range(hidden_node)]
        # น้ำหนัก (weights) สำหรับเชื่อมต่อระหว่าง hidden layer และ output layer
        self.weg2 = [[random.uniform(-1, 1) for _ in range(hidden_node)] 
                   for _ in range(output_node)]
        self.bias2 = [random.uniform(-1, 1) for _ in range(output_node)]
        
        # เก็บการเปลี่ยนแปลงของน้ำหนักจากรอบก่อนหน้า (momentum)
        self.previous_dweg1 = [[0.0 for _ in range(input_nodes)] for _ in range(hidden_node)]
        self.previous_dbias1 = [0.0 for _ in range(hidden_node)]
        self.previous_dweg2 = [[0.0 for _ in range(hidden_node)] for _ in range(output_node)]
        self.previous_dbias2 = [0.0 for _ in range(output_node)]
#กระบวนการ Forward Pass คำนวณค่าผลลัพธ์ที่ทำนายจากข้อมูลอินพุต 
    def forward(self, X):
        self.hidden_layer_input = [sum(self.weg1[i][j] * X[j] for j in range(self.input_nodes)) + self.bias1[i] 
                  for i in range(self.hidden_node)]
        self.a1 = [sigmoid_function(z) for z in self.hidden_layer_input]
        self.output_layer_input = [sum(self.weg2[k][i] * self.a1[i] for i in range(self.hidden_node)) + self.bias2[k]
                  for k in range(self.output_node)]
        self.a2 = [sigmoid_function(z) for z in self.output_layer_input]
        return self.a2[0]
#กระบวนการ Backward Pass อัปเดตน้ำหนักและ bias โดยใช้ backpropagation
    def backward(self, X, y_true, y_pred):
        error = y_pred - y_true           # คำนวณข้อผิดพลาด (error) โดยการหาค่าผลต่างระหว่างค่าทำนายและค่าจริง
        # คำนวณเกรเดียนต์ (delta) สำหรับ output layer
        delta2 = [error * sigmoid_derivative_func(y_pred)]
        delta1 = [sum(delta2[k] * self.weg2[k][i] for k in range(self.output_node)) * 
                 sigmoid_derivative_func(self.a1[i]) for i in range(self.hidden_node)]
        
        # อัปเดตน้ำหนักและ bias สำหรับ hidden layer
        for i in range(self.hidden_node):
            for j in range(self.input_nodes):
                # คำนวณเกรเดียนต์ของน้ำหนักและอัปเดต
                dweg1 = delta1[i] * X[j]
                self.weg1[i][j] -= self.learning_rate * dweg1 + self.momentum_rate * self.previous_dweg1[i][j]
                self.previous_dweg1[i][j] = dweg1      # เก็บการเปลี่ยนแปลงของน้ำหนักในรอบก่อนหน้า เพื่อใช้ในโมเมนตัม
            # อัปเดต bias สำหรับ hidden layer
            self.bias1[i] -= self.learning_rate * delta1[i] + self.momentum_rate * self.previous_dbias1[i]
            self.previous_dbias1[i] = delta1[i]
        # อัปเดตน้ำหนักและ bias สำหรับ output layer
        for k in range(self.output_node):
            for i in range(self.hidden_node):
                # คำนวณเกรเดียนต์ของน้ำหนักและอัปเดต
                dweg2 = delta2[k] * self.a1[i]
                self.weg2[k][i] -= self.learning_rate * dweg2 + self.momentum_rate * self.previous_dweg2[k][i]
                self.previous_dweg2[k][i] = dweg2
            # อัปเดต bias สำหรับ output layer
            self.bias2[k] -= self.learning_rate * delta2[k] + self.momentum_rate * self.previous_dbias2[k]
            self.previous_dbias2[k] = delta2[k]
        
        return error ** 2
    
# ฟังก์ชันการทำ Cross-Validation สำหรับการจำแนกประเภท
def class_c_vali(data, k=10, hidden_node=4, learning_r=0.01, momentum_r=0.8):
    if not data:
        print("No data.")
        return
    k = min(k, len(data))  # กำหนดจำนวน fold ไม่ให้มากกว่าขนาดข้อมูล
    fold_size = len(data) // k  # ขนาดของแต่ละ fold
    random.shuffle(data)
    all_confusion = []    # รายการสำหรับเก็บ confusion matrix ของแต่ละ fold
    # เริ่มทำ k-fold cross-validation
    for i in range(k):
        value_data = data[i*fold_size : (i+1)*fold_size]
        train_data = data[:i*fold_size] + data[(i+1)*fold_size:]

        mlp = MLP_Classification(hidden_node=hidden_node)
        mlp.learning_rate = learning_r
        mlp.momentum_rate = momentum_r
        # การฝึก Training
        for epoch in range(mlp.epoch):
            total_error = 0
            random.shuffle(train_data)
            for sample in train_data:
                X = sample[:2]
                y_true = sample[2]
                y_pred = mlp.forward(X)
                error = mlp.backward(X, y_true, y_pred)
                total_error += error
            if total_error / len(train_data) < 1e-6:
                break
        # การทดสอบ Testing
        y_true, y_pred = [] , []
        for sample in value_data:
            X = sample[:2]
            true_class = sample[2]
            out = mlp.forward(X)
            pred_class = 1 if out >= 0.5 else 0    # เปลี่ยนผลลัพธ์เป็น 0 หรือ 1
            y_pred.append(pred_class)
            y_true.append(true_class)

        CM = generate_confusion_matrix(y_true, y_pred)
        all_confusion.append(CM)
        # true_positive = TP false_negative FN , false_positive FP, true_negative= TN
        print(f"Fold {i+1} Confusion Matrix ==>  TP: {CM[0][0]} FN: {CM[0][1]} FP: {CM[1][0]} TN:{CM[1][1]}")
        print(f" [ {CM[0][0]}  {CM[0][1]} ] \n [ {CM[1][0]}  {CM[1][1]} ] \n")
    
    # คำนวณค่าเฉลี่ยความแม่นยำ
    average_accuracy = sum((CM[0][0] + CM[1][1]) / sum(sum(row) for row in CM) for CM in all_confusion ) / len(all_confusion)
    
    # ตรวจสอบว่า average_accuracy เป็นค่าที่สามารถใช้ในการแสดงผลได้หรือไม่
    if isinstance(average_accuracy, (int, float)):
        print(f"Average Accuracy = {(average_accuracy * 100):.4f} %")
    else:
        print("Error: Invalid average accuracy value")


# -============= TEST ==================-

# ข้อ1
def run_predictflood():
    print("\n----- ข้อ1 ) Predict ระดับน้ำ -----")
    
    data = load_flood_data_set("flood_data.txt")
    normalize_data, _ = normalize(data)
    # กำหนดค่า
    K_Fold_Cross_Validation(normalize_data, k=10, hidden_node=5, learning_r=0.01, momentum_r=0.9)
    print("\n---------------------------------")
# ข้อ2
def run_classification():
    print("\n----- ข้อ2 ) Classification  -----")
    
    cross_data = load_cross_data_set("cross.txt")
    if not cross_data:
        print("Failed")
        return
    # กำหนดค่า
    configs = [
        {"hidden_node": 4, "learning_r": 0.01, "momentum_r": 0.9} , {"hidden_node": 5, "learning_r": 0.03, "momentum_r": 0.8},
    ]
    
    for config in configs:
        print(f"\nConfig: hidden_node={config['hidden_node']}, learning_rate={config['learning_r']}, momentum_rate={config['momentum_r']}")
        class_c_vali(
            cross_data, k=10 , 
            hidden_node=config['hidden_node'],   learning_r=config['learning_r'],   momentum_r=config['momentum_r']
        )
        
run_predictflood()
run_classification()
